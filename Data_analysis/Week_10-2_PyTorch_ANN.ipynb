{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week 10-2 PyTorch_ANN.ipynb","private_outputs":true,"provenance":[{"file_id":"13Kr44TUrLrmJq-M6IOTC4fAuks4qWZmT","timestamp":1635932296268}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPVSjpaog6BxPHRm5rruXcM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VskMijOr0wOe"},"source":["import numpy as np\n","import torch\n","from torch import nn\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-BohvQ4k1BE"},"source":["## Logistic Regression (PyTorch)"]},{"cell_type":"code","metadata":{"id":"UOEM8HG9200q"},"source":["n_data = torch.ones(1000, 2)\n","\n","X0 = torch.normal(2 * n_data, 1)\n","y0 = torch.zeros(1000)\n","X1 = torch.normal(-2 * n_data, 1)\n","y1 = torch.ones(1000)\n","\n","train_X = np.vstack([X0, X1])\n","train_y = np.vstack([y0, y1]).reshape(-1, 1)\n","\n","C1 = np.where(train_y == True)[0]\n","C0 = np.where(train_y == False)[0]\n","\n","# Define train_X, train_y for torch\n","train_X, train_y = torch.from_numpy(train_X).float(), torch.from_numpy(train_y).float() \n","\n","print(train_X.shape, train_y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4XNzmNfu_3L"},"source":["plt.figure(figsize = (10,8))\n","plt.plot(train_X[C1,0], train_X[C1,1], 'ro', alpha = 0.3, label='C1')\n","plt.plot(train_X[C0,0], train_X[C0,1], 'bo', alpha = 0.3, label='C0')\n","plt.xlabel(r'$x_1$', fontsize = 15)\n","plt.ylabel(r'$x_2$', fontsize = 15)\n","plt.legend(loc = 1, fontsize = 12)\n","plt.axis('equal')\n","plt.ylim([-5,5])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmEw9Vk5mdv6"},"source":["from torch.utils.data import DataLoader, TensorDataset\n","\n","def load_array(data_arrays, batch_size, is_train = True):\n","    # Define dataset and dataloader\n","    dataset = TensorDataset(*data_arrays)\n","    dataloader = DataLoader(dataset = dataset, \n","                            batch_size = batch_size,\n","                            shuffle = is_train)\n","    return dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0u13o66meWm"},"source":["data_iter = load_array((train_X, train_y), batch_size=len(train_y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCjW11cUncIX"},"source":["class LogisticRegressionModel(torch.nn.Module):\n","    # Define init, forward method\n","    def __init__(self):\n","        super(LogisticRegressionModel, self).__init__()\n","        self.layer = torch.nn.Linear(2, 1)\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","    def forward(self, inputs):\n","        outputs = self.layer(inputs)\n","        return self.sigmoid(outputs)\n","\n","model_logR = LogisticRegressionModel()\n","\n","if torch.cuda.is_available():\n","    train_X, train_y = train_X.cuda(), train_y.cuda()\n","    model_logR.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3RkCQRXoYGl"},"source":["print(model_logR.layer.weight.data)\n","print(model_logR.layer.bias.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PlWzNa1Rodve"},"source":["# Define SGD optimizer \n","optimizer_logR = torch.optim.SGD(model_logR.parameters(), lr = 0.05)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCH8SooSlLjS"},"source":["model_logR.parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUvM68hHolNf"},"source":["num_epochs = 200\n","loss_graph_logR = []\n","\n","for epoch in range(num_epochs):\n","    for X, y in data_iter:\n","        # training with predict, loss, zero_grad, backward, step\n","        predict_logR = model_logR(train_X)\n","        loss_logR = torch.nn.functional.binary_cross_entropy(predict_logR, train_y)\n","        optimizer_logR.zero_grad()\n","        loss_logR.backward()\n","        optimizer_logR.step()\n","    loss_graph_logR.append(torch.nn.functional.binary_cross_entropy(model_logR(train_X), train_y))\n","plt.plot(loss_graph_logR)\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8zHOMYlxv_oO"},"source":["w1 = model_logR.layer.weight[0][0].item()\n","w2 = model_logR.layer.weight[0][1].item()\n","b = model_logR.layer.bias.item()\n","\n","print(w1, w2, b)\n","\n","xp = np.arange(-4, 4, 0.01).reshape(-1, 1)\n","yp = - w1 / w2 * xp - b / w2\n","\n","train_X, train_y = train_X.cpu(), train_y.cpu()\n","\n","plt.figure(figsize = (10,8))\n","plt.plot(train_X[C1,0], train_X[C1,1], 'ro', alpha = 0.3, label='C1')\n","plt.plot(train_X[C0,0], train_X[C0,1], 'bo', alpha = 0.3, label='C0')\n","plt.plot(xp, yp, 'g', linewidth = 3, label = 'Logistic Regression')\n","plt.xlabel(r'$x_1$', fontsize = 15)\n","plt.ylabel(r'$x_2$', fontsize = 15)\n","plt.legend(loc = 1, fontsize = 12)\n","plt.axis('equal')\n","plt.ylim([-4,4])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"udWtDT7wP1yK"},"source":["## MNIST_MLP"]},{"cell_type":"code","metadata":{"id":"yLNAIgaTP4vJ"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FHX_T8JpP7pR"},"source":["BATCH_SIZE = 32\n","EPOCHS = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"35_e-3LwQVcn"},"source":["train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n","                               train = True,\n","                               download = True,\n","                               transform = transforms.ToTensor())\n","\n","test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n","                              train = False,\n","                              transform = transforms.ToTensor())\n","\n","# Define train_loader, Test_loader\n","train_loader = DataLoader(dataset=train_dataset,\n","                          batch_size = BATCH_SIZE,\n","                          shuffle = True)\n","\n","test_loader = DataLoader(dataset=test_dataset,\n","                         batch_size = BATCH_SIZE,\n","                         shuffle = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HezHz8TMQo6l"},"source":["for (X_train, y_train) in train_loader:\n","    print('X_train:', X_train.size(), 'type:', X_train.type())\n","    print('y_train:', y_train.size(), 'type:', y_train.type())\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VOhsHOuhQrnH"},"source":["pltsize = 1\n","plt.figure(figsize=(10 * pltsize, pltsize))\n","for i in range(10):\n","    plt.subplot(1, 10, i + 1)\n","    plt.axis('off')\n","    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n","    plt.title('Class: ' + str(y_train[i].item()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Evy-mJ30Q-QE"},"source":["class NeuralNet(torch.nn.Module):\n","    # Define init and forward method\n","    def __init__(self):\n","        super(NeuralNet, self).__init__()\n","        self.fclayer1 = torch.nn.Linear(28 * 28, 512)\n","        self.fclayer2 = torch.nn.Linear(512, 256)\n","        self.fclayer3 = torch.nn.Linear(256, 10)\n","        self.dropout_prob = 0.5\n","        self.batch_norm1 = nn.BatchNorm1d(512)\n","        self.batch_norm2 = nn.BatchNorm1d(256)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28 * 28)\n","        x = self.fclayer1(x)\n","        x = self.batch_norm1(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n","        x = self.fclayer2(x)\n","        x = self.batch_norm2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, training=self.training, p = self.dropout_prob)\n","        x = self.fclayer3(x)\n","        x = F.log_softmax(x, dim = 1)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUdXJmHLSzWN"},"source":["if torch.cuda.is_available():\n","    DEVICE = torch.device('cuda')\n","else:\n","    DEVICE = torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ubkjhq3iSJf4"},"source":["model = NeuralNet().to(DEVICE)\n","\n","# Define optimizer, loss\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Do6IcrwS5nq"},"source":["def train(model, train_loader, optimizer, log_interval):\n","    model.train()\n","    for batch_idx, (image, label) in enumerate(train_loader):\n","        image = image.to(DEVICE)\n","        label = label.to(DEVICE)\n","        # training with output, loss, zero_grad, backward, step\n","        output = model(image)\n","        loss = criterion(output, label)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","   \n","        if batch_idx % log_interval == 0:\n","            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n","                epoch, batch_idx * len(image), \n","                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n","                loss.item())) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUYu2Z1lTktu"},"source":["def evaluate(model, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for image, label in test_loader:\n","            image = image.to(DEVICE)\n","            label = label.to(DEVICE)\n","            # Evaluate with output, test_loss, prediction, correct\n","            output = model(image)\n","            test_loss += criterion(output, label).item()\n","            prediction = output.max(1, keepdim = True)[1]\n","            correct += prediction.eq(label.view_as(prediction)).sum().item()\n","    \n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\n","    return test_loss, test_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-v6OyZcbUQYX"},"source":["for epoch in range(1, EPOCHS + 1):\n","    train(model, train_loader, optimizer, log_interval = 200)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n","        epoch, test_loss, test_accuracy))"],"execution_count":null,"outputs":[]}]}